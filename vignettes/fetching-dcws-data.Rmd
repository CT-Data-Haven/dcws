---
title: "Fetching DCWS data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Fetching DCWS data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = TRUE,
	collapse = TRUE,
	comment = "#>"
)
```

This package has a dataset of all the data extracted from all the crosstabs for every survey wave that I had access to as of January 2021, a total of 97 files. It's all been cleaned up so that names, categories, and groups should be standardized to match across every location and year, and should be the same in the weights as in the data tables themselves. Every data point has its corresponding question text and code. 

When it's all combined and unnested, there are more than 757,000 rows of data, and no one needs all of that. Instead, use the function `fetch_cws` to pull out just the data you need, whether that's just one location across multiple years, multiple locations, a single question, or whatever.

Here are some pretty common examples of how we might work with DCWS data using this package, `cwi`, and some tidyverse. I'll mostly use some of the more straightforward questions that we ask every year, with Greater New Haven as an example because I know most of our demographic groups are available.

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(dcws)
library(cwi)
library(forcats)
```

A common thing we'll present is a question represented as a single number (percent responding "yes", percent responding "strongly agree" or "somewhat agree", etc.) for the state, the region, a couple groups within that region, and large towns in the region. For this, I'll use named arguments for the year and categories of respondents, plus some of the open-ended filtering. I'll start out looking at food insecurity, since it's a simple yes or no. Since I'm doing this a couple times, I'll write a (pretty crappy) function. 


```{r}
calc_food <- function(data) {
  data %>%
    # get order that makes sense for this subset
    # mutate(group = fct_inorder(fct_drop(group))) %>% 
    sub_nonanswers() %>% # remove don't know/refused & rescale values
    filter(group != "Other race",
           response == "Yes") %>% # other race is only available in 2015
    select(year, name, category, group, value) %>%
    mutate(value = round(value, digits = 2))
}
```

## Analysis

### One question, one year, few categories

```{r food1}
food_21 <- fetch_cws(grepl("Have there been times .+ food", question), 
          .name = "Greater New Haven", .category = c("Total", "Race/Ethnicity", "Gender"),
          .year = 2021, .unnest = TRUE) %>%
  calc_food()

knitr::kable(food_21)
```


### One question, several years, one location


```{r food2}
food_trend <- fetch_cws(grepl("Have there been times .+ food", question), 
          .name = "Greater New Haven", .category = c("Total", "Race/Ethnicity", "Gender"),
          .unnest = TRUE) %>%
  calc_food()

knitr::kable(food_trend)
```



### One question, one year, compare groups and locations

I want just the location-wide values for towns in Greater New Haven, or by race for Greater New Haven. I could do this filtering inside `fetch_cws` if I wanted to dig into the nested data with `purrr`, but I don't, so I'll just be a little redundant.

```{r food3}
food_towns <- fetch_cws(grepl("Have there been times .+ food", question), 
          .name = c("Greater New Haven", cwi::regions$`Greater New Haven`),
          .year = 2021, .unnest = TRUE) %>%
  filter((name == "Greater New Haven" & category %in% c("Total", "Race/Ethnicity")) |
           (group %in% cwi::regions$`Greater New Haven`)) %>%
  calc_food() %>%
  mutate(category = as_factor(ifelse(name == "Greater New Haven", 
                                     as.character(category),
                                     "By town")))

knitr::kable(food_towns)
```

### Several questions with same responses

Usually I like to analyze questions separately because they might not have the same set of responses, but for a bank of related questions you can do them all at once. In 2020 and 2021 we added a question about trust in several types of institutions. I can't remember the codes for them but they all have "trust" in the question text.

```{r lookup}
fetch_cws(grepl("[Tt]rust", question), .year = 2021) %>%
  distinct(code, question)
```

Oh duh, the codes are all "TRUST" and then a letter! If this type of lookup table / function would be useful, I can add one, but it's pretty simple to do on the fly.

The responses for these questions are a great deal, a fair amount, not very much, or none at all. I'm going to collapse great deal and fair amount into one, then present just that. Each question has the same beginning text, then at the end names the institution being asked about.

```{r trust1}
trust_insts <- fetch_cws(grepl("^TRUST[A-Z]$", code), .name = "Greater New Haven", .year = 2021,
          .category = c("Total", "Age"), .unnest = TRUE) %>%
  mutate(response = fct_collapse(response, trust = c("A great deal", "A fair amount")),
         question = stringr::str_extract(question, "([\\w\\s]+)$") %>%
           trimws() %>%
           as_factor(),
         group = fct_inorder(fct_drop(group))) %>%
  group_by(category, group, question, response) %>%
  summarise(value = sum(value)) %>%
  sub_nonanswers() %>%
  filter(response == "trust") %>%
  mutate(value = round(value, digits = 2)) %>%
  ungroup()

trust_insts %>%
  tidyr::pivot_wider(id_cols = group, names_from = question) %>%
  knitr::kable()
```

Lots of ways you could chop this data up now that you've got several groups and several questions together.

### Comparing to the state

All the crosstabs include Connecticut total values to compare to. The script that extracts all the crosstab data includes these, because they're sometimes useful: in most of the tables and charts we publish of survey data for one location includes state values. The benefit is that if you want, say, Greater New Haven data, you don't have to do anything special to also have Connecticut totals. However, if you pull data for multiple locations, this would be annoyingly redundant. So I've added an argument `.drop_ct` that defaults to true, in which case Connecticut values from other locations' crosstabs are dropped before your data are returned.

With `.drop_ct = TRUE` (the default):

```{r}
fetch_cws(code == "Q1", .year = 2021, .name = "Greater New Haven", 
          .category = c("Total", "Gender"), .unnest = TRUE) %>%
  distinct(year, name, category, group)
```

With `.drop_ct = FALSE`:

```{r}
fetch_cws(code == "Q1", .year = 2021, .name = "Greater New Haven", 
          .category = c("Total", "Gender"), .unnest = TRUE, .drop_ct = FALSE) %>%
  distinct(year, name, category, group)
```

## Weights

The crosstabs each have a table of survey weights for each group, either as a standalone section at the bottom of the Excel spreadsheet or as a couple rows at the top of each question. `cwi::read_weights` now works with either of these formats; for the latter, the weights are taken from the first question (always the satisfied with your area question, which every participant receives). Just like preparing this package meant extracting all the data, there's also a stash of all the weights from all the files. These are useful for operations like collapsing multiple small groups into larger ones, usually income brackets or other groupings that may not be consistent between years or locations otherwise.

There's a function for getting weights on their own, with most of the same arguments as for getting data...

```{r wts1}
head(fetch_wts(.year = 2021, .name = "Greater New Haven", .unnest = TRUE))
```

...or you can just use the `.add_wts` argument in `fetch_cws` to do this for you. You can also use `cwi::collapse_n_wt` to help with the calculation, but basically you're collapsing several levels, then getting weighted means.

One reason for doing this is that locations might have different income brackets depending on sample size, and we've moved toward using larger income brackets in the latest wave of the survey. So in addition to sample size, you might also collapse groups so you can compare across locations or years.

For example, check out how obnoxious this is:

```{r}
satisfied_area <- fetch_cws(grepl("satisfied with the city", question), 
          .name = c("Connecticut", "Greater New Haven", "New Haven"), .unnest = TRUE, .category = c("Total", "Income"))

satisfied_area %>%
  filter(category == "Income") %>%
  distinct(year, name, group) %>%
  mutate(value = "x", id = paste(name, year)) %>%
  tidyr::pivot_wider(id_cols = group, names_from = id, values_from = value, names_sort = TRUE, values_fill = "") %>%
  arrange(group) %>%
  knitr::kable()
```

Yikes

So you'll probably want to collapse some of those, like so:

```{r wts2}
asthma18 <- fetch_cws(question == "Asthma", .year = 2018, .name = "Greater New Haven", 
          .category = c("Total", "Income", "Race/Ethnicity"), .add_wts = TRUE, .unnest = TRUE) %>%
  collapse_n_wt(year:response,
                .lvls = list("<$30K" = c("<$15K", "$15K-$30K"),
                             "$30K-$100K" = c("$30K-$50K", "$50K-$75K", "$75K-$100K"),
                             "$100K+" = c("$100K-$200K", "$200K+"))) %>%
  sub_nonanswers() %>%
  mutate(value = round(value, digits = 2)) %>%
  filter(response == "Yes")

knitr::kable(asthma18)
```



## Output

That's it! Usually I'll save a bunch of related analyses into lists of data frames, and then write those out to rds files for easy loading.
